{"cells":[{"cell_type":"markdown","metadata":{"id":"OOSjmpMw5gxy"},"source":["## __0. Download dataset__\n","**Note:** If you can't download using gdown due to limited number of downloads, please download it manually and upload it to your drive, then copy it from the drive to colab.\n","```python\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","!cp /path/to/dataset/on/your/drive .\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"94MQBB3h9WT_"},"outputs":[],"source":["# https://drive.google.com/file/d/1kc6XNqHZJg27KeBuoAoYj70_1rT92191/view?usp=sharing\n","!gdown --id 1kc6XNqHZJg27KeBuoAoYj70_1rT92191"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rb5Rh0fGAStN"},"outputs":[],"source":["!unzip -q vqa_coco_dataset.zip"]},{"cell_type":"markdown","metadata":{"id":"VljPGwOLCGoj"},"source":["## __1. Import libraries and set random seed__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KDAACoro5r38","scrolled":true},"outputs":[],"source":["!pip install timm==0.8.17.dev0\n","!pip install torchtext==0.15.1\n","!pip install torchvision==0.16.0\n","!pip install torchaudio==2.1.0\n","!pip install torch==2.1.0\n","!pip install transformers==4.27.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zwmJUbxKCPjS"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import spacy\n","import timm\n","import matplotlib.pyplot as plt\n","\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from transformers import ViTModel, ViTImageProcessor\n","from transformers import AutoTokenizer, RobertaModel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YctOB7QASJvK"},"outputs":[],"source":["def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","seed = 59\n","set_seed(seed)"]},{"cell_type":"markdown","metadata":{"id":"Iu20gzLxCu2y"},"source":["## __2. Read dataset__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOxeYUBvCSki"},"outputs":[],"source":["train_data = []\n","train_set_path = 'vaq2.0.TrainImages.txt'\n","\n","with open(train_set_path, \"r\") as f:\n","    lines = f.readlines()\n","    for line in lines:\n","        temp = line.split('\\t')\n","        qa = temp[1].split('?')\n","\n","        if len(qa) == 3:\n","            answer = qa[2].strip()\n","        else:\n","            answer = qa[1].strip()\n","\n","        data_sample = {\n","            'image_path': temp[0][:-2],\n","            'question': qa[0] + '?',\n","            'answer': answer\n","        }\n","        train_data.append(data_sample)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d6Qf8EMA-gjK"},"outputs":[],"source":["classes = set([sample['answer'] for sample in train_data])\n","label2idx = {\n","    cls_name: idx for idx, cls_name in enumerate(classes)\n","}\n","idx2label = {\n","    idx: cls_name for idx, cls_name in enumerate(classes)\n","}\n","print(idx2label)"]},{"cell_type":"markdown","metadata":{"id":"9UznC-J7FVTf"},"source":["## __3. Create Pytorch dataset__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fXAFcSjnGlZF"},"outputs":[],"source":["### YOUR CODE GOES HERE\n","class VQADataset(Dataset):\n","    def __init__(\n","        self,\n","        data,\n","        label2idx,\n","        img_feature_extractor,\n","        text_tokenizer,\n","        device,\n","        transforms=None,\n","        img_dir='val2014-resised'\n","    ):\n","        pass\n","\n","    def __len__(self):\n","        return None\n","\n","    def __getitem__(self, index):\n","\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X8CqwfHESJvM"},"outputs":[],"source":["data_transform = transforms.Compose([\n","    transforms.Resize(size=(224, 224)),\n","    transforms.CenterCrop(size=180),\n","    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.GaussianBlur(3),\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NmyFmD_GL-BQ"},"outputs":[],"source":["img_feature_extractor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n","text_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","train_dataset = VQADataset(\n","    train_data,\n","    label2idx=label2idx,\n","    img_feature_extractor=img_feature_extractor,\n","    text_tokenizer=text_tokenizer,\n","    device=device,\n","    transforms=data_transform\n",")\n","val_dataset = VQADataset(\n","    val_data,\n","    label2idx=label2idx,\n","    img_feature_extractor=img_feature_extractor,\n","    text_tokenizer=text_tokenizer,\n","    device=device\n",")\n","test_dataset = VQADataset(\n","    test_data,\n","    label2idx=label2idx,\n","    img_feature_extractor=img_feature_extractor,\n","    text_tokenizer=text_tokenizer,\n","    device=device\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AToEIVctirQK","scrolled":true},"outputs":[],"source":["train_batch_size = 256\n","test_batch_size = 32\n","\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=train_batch_size,\n","    shuffle=True\n",")\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=test_batch_size,\n","    shuffle=False\n",")\n","test_loader = DataLoader(\n","    test_dataset,\n","    batch_size=test_batch_size,\n","    shuffle=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"026aiVfoSJvM"},"outputs":[],"source":["img_dir='val2014-resised/'\n","img_to_show = 10\n","shift = 10\n","test_imgs = [\n","    os.path.join(img_dir, sample['image_path']) for sample in train_data[shift:shift+img_to_show]\n","]\n","fig, axes = plt.subplots(nrows=1, ncols=img_to_show, figsize=(20, 4))\n","for ax, img_path in zip(axes, test_imgs):\n","    img_pil = Image.open(img_path).convert('RGB')\n","    preprocessed_img = data_transform(img_pil)\n","    ax.imshow(preprocessed_img)\n","    ax.axis('off')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"fOcnB225J90K"},"source":["## __4. Create VQA model__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PTI-Byg3syEu"},"outputs":[],"source":["### YOUR CODE GOES HERE\n","class VisualEncoder(nn.Module):\n","    def __init__(self):\n","        super(VisualEncoder, self).__init__()\n","        pass\n","\n","    def forward(self, inputs):\n","\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZ42bHClszGG"},"outputs":[],"source":["### YOUR CODE GOES HERE\n","class TextEncoder(nn.Module):\n","    def __init__(self):\n","        super(TextEncoder, self).__init__()\n","        pass\n","\n","    def forward(self, inputs):\n","\n","        return None"]},{"cell_type":"code","source":["### YOUR CODE GOES HERE\n","class Classifier(nn.Module):\n","    def __init__(\n","        self,\n","        hidden_size=512,\n","        dropout_prob=0.2,\n","        n_classes=2\n","    ):\n","        super(Classifier,self).__init__()\n","        pass\n","\n","    def forward(self,x):\n","\n","        return None"],"metadata":{"id":"NiS3Uff4SyTO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IjrAeibMtg5H"},"outputs":[],"source":["### YOUR CODE GOES HERE\n","class VQAModel(nn.Module):\n","    def __init__(\n","        self,\n","        visual_encoder,\n","        text_encoder,\n","        classifier\n","    ):\n","        super(VQAModel, self).__init__()\n","        pass\n","\n","\n","    def forward(self, image, answer):\n","\n","        return None\n","\n","    def freeze(self, visual=True, textual=True, clas=False):\n","        pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"obxbBWAbkg2A"},"outputs":[],"source":["n_classes = len(classes)\n","hidden_size = 256\n","dropout_prob = 0.2\n","\n","text_encoder = TextEncoder().to(device)\n","visual_encoder = VisualEncoder().to(device)\n","classifier = Classifier(\n","    hidden_size=hidden_size,\n","    dropout_prob=dropout_prob,\n","    n_classes=n_classes\n",").to(device)\n","\n","model = VQAModel(\n","    visual_encoder=visual_encoder,\n","    text_encoder=text_encoder,\n","    classifier=classifier\n",").to(device)\n","model.freeze()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZzhJzxrGmWSO"},"outputs":[],"source":["inputs = next(iter(train_loader))\n","\n","model.eval()\n","with torch.no_grad():\n","    image = inputs['image']\n","    question = inputs['question']\n","    output = model(image, question)\n","    print(output.shape)"]},{"cell_type":"markdown","metadata":{"id":"OmPEn5s-mjJe"},"source":["## __5. Training__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pBIA8pH0rEHF"},"outputs":[],"source":["def evaluate(model, dataloader, criterion):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    losses = []\n","    with torch.no_grad():\n","        for idx, inputs in enumerate(dataloader):\n","            images = inputs['image']\n","            questions = inputs['question']\n","            labels = inputs['label']\n","            outputs = model(images, questions)\n","            loss = criterion(outputs, labels)\n","            losses.append(loss.item())\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    loss = sum(losses) / len(losses)\n","    acc = correct / total\n","\n","    return loss, acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MxNXk8_xrkon"},"outputs":[],"source":["def fit(\n","    model,\n","    train_loader,\n","    val_loader,\n","    criterion,\n","    optimizer,\n","    scheduler,\n","    epochs\n","):\n","    train_losses = []\n","    val_losses = []\n","\n","    for epoch in range(epochs):\n","        batch_train_losses = []\n","\n","        model.train()\n","        for idx, inputs in enumerate(train_loader):\n","            images = inputs['image']\n","            questions = inputs['question']\n","            labels = inputs['label']\n","\n","            optimizer.zero_grad()\n","            outputs = model(images, questions)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            batch_train_losses.append(loss.item())\n","\n","        train_loss = sum(batch_train_losses) / len(batch_train_losses)\n","        train_losses.append(train_loss)\n","\n","        val_loss, val_acc = evaluate(\n","            model, val_loader,\n","            criterion\n","        )\n","        val_losses.append(val_loss)\n","\n","        print(f'EPOCH {epoch + 1}:\\tTrain loss: {train_loss:.4f}\\tVal loss: {val_loss:.4f}\\tVal Acc: {val_acc}')\n","\n","        scheduler.step()\n","\n","    return train_losses, val_losses"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6hj65HGor2pv"},"outputs":[],"source":["lr = 1e-3\n","epochs = 50\n","scheduler_step_size = epochs * 0.8\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer = torch.optim.Adam(\n","    model.parameters(),\n","    lr=lr\n",")\n","scheduler = torch.optim.lr_scheduler.StepLR(\n","    optimizer,\n","    step_size=scheduler_step_size,\n","    gamma=0.1\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z5Qmw6efr5HG"},"outputs":[],"source":["train_losses, val_losses = fit(\n","    model,\n","    train_loader,\n","    val_loader,\n","    criterion,\n","    optimizer,\n","    scheduler,\n","    epochs\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xIobHWIptBQm"},"outputs":[],"source":["fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n","ax[0].plot(train_losses)\n","ax[0].set_title('Training Loss')\n","ax[0].set_xlabel('Epoch')\n","ax[0].set_ylabel('Loss')\n","ax[1].plot(val_losses, color='orange')\n","ax[1].set_title('Val Loss')\n","ax[1].set_xlabel('Epoch')\n","ax[1].set_ylabel('Loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"JPFgeiArr6zp"},"source":["## __6. Evaluation__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"psvbHQpis5bm"},"outputs":[],"source":["val_loss, val_acc = evaluate(\n","    model,\n","    val_loader,\n","    criterion\n",")\n","test_loss, test_acc = evaluate(\n","    model,\n","    test_loader,\n","    criterion\n",")\n","\n","print('Evaluation on val/test dataset')\n","print('Val accuracy: ', val_acc)\n","print('Test accuracy: ', test_acc)"]},{"cell_type":"markdown","metadata":{"id":"0nazA0XFSJvO"},"source":["## __7. Inference__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3_ZABqebSJvO"},"outputs":[],"source":["idx = 10\n","img_name = test_data[idx]['image_path']\n","img_path = os.path.join(img_dir, img_name)\n","question = test_data[idx]['question']\n","label = test_data[idx]['answer']\n","\n","question_processed = text_tokenizer(\n","    question,\n","    padding=\"max_length\",\n","    max_length=20,\n","    truncation=True,\n","    return_tensors=\"pt\"\n",").to(device)\n","\n","\n","img = Image.open(img_path).convert(\"RGB\")\n","img_processed = img_feature_extractor(images=img, return_tensors=\"pt\").to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    output = model(img_processed, question_processed)\n","    pred = torch.argmax(output, dim=1).item()\n","\n","plt.figure(figsize=(6, 6))\n","plt.imshow(np.array(img))\n","plt.axis('off')\n","plt.show()\n","\n","print(f'Question: {question}')\n","print(f'Groundtruth: {label}')\n","print(f'Predicted: {idx2label[pred]}')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"thangdd_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}